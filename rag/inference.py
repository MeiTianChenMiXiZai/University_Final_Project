# -*- coding: utf-8 -*-
import os
import re

from rag.llm.config import OLLAMA_CONFIG
from rag.llm.ollama_client import OllamaClient
from rag.llm.policy_engine import PolicyQAEngine
from rag.nlp.search import SearchEngine
from rag.nlp import rag_tokenizer
from rag.match import process_pdf_highlight

# åˆå§‹åŒ– RAG ç»„ä»¶
search_engine = SearchEngine(score_threshold=0.5)  # è®¾å®šæœ€å°ç›¸å…³æ€§
client = OllamaClient(OLLAMA_CONFIG)
engine = PolicyQAEngine(client)


# **OCR ç›¸å…³è·¯å¾„**
pdf_base_path = "C:/Users/ROG/PycharmProjects/final/data/policy"  # å­˜æ”¾æ”¿ç­–æ–‡ä»¶çš„ PDF ç›®å½•
# output_dir = "C:/Users/ROG/PycharmProjects/final/rag/ocr_results"  # OCR è§£æåçš„å›¾ç‰‡å­˜æ”¾ç›®å½•
output_dir = "C:/Users/ROG/PycharmProjects/final/ocr_results"  # OCR è§£æåçš„å›¾ç‰‡å­˜æ”¾ç›®å½•

def filter_top_results(search_results):
    """
    é€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ®µï¼š
    - é€‰å–å¾—åˆ† >= æœ€é«˜åˆ†çš„ 50%ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
    - é¿å…é€‰å¤ªå°‘å¯¼è‡´ä¿¡æ¯ä¸å…¨
    - é¿å…é€‰å¤ªå¤šå¯¼è‡´ LLM è¯¯è§£
    """
    if not search_results:
        return []

    max_score = max(result["æœç´¢åˆ†æ•°"] for result in search_results)
    threshold = max_score * 0.5 # è®¾å®š 50% é˜ˆå€¼

    filtered_results = [res for res in search_results if res["æœç´¢åˆ†æ•°"] >= threshold]

    print(f"ğŸ” **é€‰å– {len(filtered_results)} ä¸ªç›¸å…³æ–‡æ®µ (é˜ˆå€¼: {threshold:.3f})**")
    return filtered_results



def format_search_results(search_results):
    """
    æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œç”Ÿæˆæ¸…æ™°çš„ LLM è¾“å…¥
    """
    formatted_texts = []
    for result in search_results:
        doc_name = result["æ–‡ä»¶"]
        score = result["æœç´¢åˆ†æ•°"]
        sentences = "\n".join([f"({round(sent[1], 2)}) {sent[0]}" for sent in result["ç›¸å…³å†…å®¹"]])

        formatted_text = f"ğŸ“„ æ–‡ä»¶: {doc_name} (ç›¸å…³æ€§: {score})\n{sentences}"
        formatted_texts.append(formatted_text)

    return "\n\n".join(formatted_texts)


def extract_keywords_nlp(question: str) -> tuple[list[str], dict[str, list[str]]]:
    """
    ä½¿ç”¨ NLP è§„åˆ™æå–å…³é”®è¯ï¼Œå¹¶è¿”å›ç©ºçš„è¿‘ä¹‰è¯å­—å…¸
    - å…³é”®è¯ï¼šä½¿ç”¨ `rag_tokenizer` è¿›è¡Œåˆ†è¯
    - è¿‘ä¹‰è¯ï¼šä¸ºç©ºå­—å…¸ {}
    """
    text_tokens = set(rag_tokenizer.tokenize(question).split())  # ä½¿ç”¨ rag_tokenizer è¿›è¡Œåˆ†è¯
    return list(text_tokens), {}  # NLP ç‰ˆä¸æä¾›è¿‘ä¹‰è¯æ‰©å±•

def extract_keywords(question: str, use_llm=True, retries=2) -> tuple[list[str], dict[str, list[str]]]:
    """
    æå–æœç´¢å…³é”®è¯ï¼Œå¹¶è¿›è¡Œè¿‘ä¹‰è¯æ‰©å±•ï¼š
    - `use_llm=True`ï¼šä¼˜å…ˆä½¿ç”¨ LLM æå–å…³é”®è¯ + è¿‘ä¹‰è¯
    - `use_llm=False`ï¼šä½¿ç”¨ NLP è§„åˆ™åˆ†è¯
    - LLM å¤±è´¥ä¸¤æ¬¡åï¼Œè‡ªåŠ¨å›é€€åˆ° NLP
    """
    if use_llm:
        for attempt in range(retries):
            try:
                result = client.extract_keywords(question)  # LLM æå–å…³é”®è¯ + è¿‘ä¹‰è¯
                if isinstance(result, tuple) and len(result) == 2:
                    keywords, synonyms = result  # ç¡®ä¿è¿”å›æ ¼å¼æ­£ç¡®
                    if isinstance(keywords, list) and isinstance(synonyms, dict):
                        return keywords, synonyms  # LLM ç»“æœæœ‰æ•ˆï¼Œè¿”å›

            except Exception as e:
                print(f"âš ï¸ LLM ç¬¬ {attempt + 1} æ¬¡å…³é”®è¯æå–å¤±è´¥ï¼Œé”™è¯¯: {e}ï¼Œé‡æ–°è¯·æ±‚...")

        print("âš ï¸ LLM è¿ç»­å¤±è´¥ï¼Œå›é€€åˆ° NLP åˆ†è¯...")

    return extract_keywords_nlp(question)  # NLP è§„åˆ™åˆ†è¯ï¼ˆå›é€€æ–¹æ¡ˆï¼‰




def answer_policy_question(question, top_k=5):
    """
    1. è§£æé—®é¢˜ï¼Œæå–å…³é”®è¯
    2. ä½¿ç”¨ RAG è¿›è¡Œæœç´¢
    3. ç»“åˆ LLM ç”Ÿæˆç­”æ¡ˆ
    4. é«˜äº®åŒ¹é…çš„æ”¿ç­–å†…å®¹
    """
    print(f"\nğŸ” **åŸå§‹æŸ¥è¯¢:** {question}")

    # è·å–å…³é”®è¯å’Œè¿‘ä¹‰è¯
    keywords, synonyms = extract_keywords(question, use_llm=True)

    # å…³é”®è¯ä¸ºç©ºæ—¶ï¼Œè¿”å›æç¤º
    if not keywords:
        return {
            "raw_question": question,
            "error": "âŒ æ— æ³•è§£æé—®é¢˜ï¼Œè¯·å°è¯•æ›´å…·ä½“çš„æé—®ã€‚"
        }

    # æ„é€ æœ€ç»ˆæœç´¢ Queryï¼ˆåŒ…å«è¿‘ä¹‰è¯ï¼‰
    expanded_query = set(keywords)
    for key, syns in synonyms.items():
        expanded_query.update(syns)

    search_query = " ".join(expanded_query)
    print(f"ğŸ” **ä¼˜åŒ–åçš„æœç´¢ Query:** {search_query}")

    search_results = search_engine.search(search_query, top_k)

    if not search_results:
        return {
            "raw_question": question,
            "optimized_query": search_query,
            "error": "âŒ æœªèƒ½æ‰¾åˆ°ç›¸å…³æ”¿ç­–ï¼Œè¯·å°è¯•æ›´å…·ä½“çš„é—®é¢˜ã€‚"
        }

    print("\nğŸ“„ **æœç´¢åˆ°æ”¿ç­–å†…å®¹:**")
    print(search_results)

    # é€‰å–å¾—åˆ†æœ€é«˜çš„ 80% ä»¥å†…çš„æ–‡æ®µ
    filtered_results = filter_top_results(search_results)

    # æ ¼å¼åŒ–æ£€ç´¢å†…å®¹
    formatted_docs = format_search_results(filtered_results)
    print("\nğŸ“„ **æœ€ç»ˆç”¨äºå›ç­”çš„æ”¿ç­–å†…å®¹:**")
    print(formatted_docs)

    # è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
    raw_answer = engine.answer_question(question, filtered_results)

    print("\nğŸ“ **æ€è€ƒå›ç­”:**")
    print(raw_answer)

    # æ¸…ç† LLM è¾“å‡ºï¼Œå»æ‰ <think> æ ‡ç­¾
    cleaned_answer = re.sub(r"<think>.*?</think>", "", raw_answer, flags=re.DOTALL).strip()
    print("\nğŸ“ **æœ€ç»ˆå›ç­”:**")
    print(cleaned_answer)

    # é«˜äº®åŒ¹é…çš„æ”¿ç­–å†…å®¹
    referenced_texts = cleaned_answer.split("\n")
    matched_passages = process_pdf_highlight(pdf_base_path, referenced_texts, filtered_results, output_dir)

    return {
        "raw_question": question,
        "optimized_query": search_query,
        "sorted_results": search_results,
        "filtered_results": filtered_results,
        "llm_thinking": raw_answer,
        "final_answer": cleaned_answer,
        "reference_images": matched_passages
    }

'''
# **æµ‹è¯•ä»£ç **
if __name__ == "__main__":
    test_questions = [
        "é€‰ä¿®è¯¾ç¨‹å¯ä»¥è¡¥è€ƒå—"
    ]

    for question in test_questions:
        answer, highlighted_info = answer_policy_question(question)
        print("\nğŸ” **åŒ¹é…åˆ°çš„æ”¿ç­–å†…å®¹:**")
        print(highlighted_info)

'''